{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Classifiers with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T13:11:41.988722Z",
     "start_time": "2021-03-20T13:11:40.825353Z"
    },
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# produce vector inline graphics\n",
    "from IPython.display import Code, display\n",
    "from ipywidgets import interact\n",
    "from sklearn import datasets, neighbors, preprocessing, tree\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# from wittgenstein import RIPPER\n",
    "from util import plot_decision_regions\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we consider the binary classification problem on the [breast cancer dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:34.175973Z",
     "start_time": "2021-02-01T12:50:34.133315Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# load the dataset from sklearn\n",
    "dataset = datasets.load_breast_cancer()\n",
    "\n",
    "# create a DataFrame to help further analysis\n",
    "df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "df[\"target\"] = dataset.target\n",
    "df.target = df.target.astype(\"category\")\n",
    "df.target.cat.categories = dataset.target_names\n",
    "df  # display an overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to train a classifier to diagnose whether a breast mass is malignant or benign. The target class distribution is shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:34.331940Z",
     "start_time": "2021-02-01T12:50:34.177731Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "display(df.target.value_counts())\n",
    "df.target.value_counts().plot(kind=\"bar\", title=\"counts of different classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input features are characteristics of cell images obtained by [fine needle analysis (FNA)](https://en.wikipedia.org/wiki/Fine-needle_aspiration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T13:11:42.000193Z",
     "start_time": "2021-03-20T13:11:41.990577Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<a href=\"https://www.researchgate.net/figure/FNA-biopsy-samples-of-benign-left-and-malignant-center-and-right-breast-tumor-cells_fig1_261959799\"><img src=\"https://www.researchgate.net/profile/Carsten_Eickhoff/publication/261959799/figure/fig1/AS:296624837414914@1447732280878/FNA-biopsy-samples-of-benign-left-and-malignant-center-and-right-breast-tumor-cells.png\" alt=\"FNA biopsy samples of benign (left) and malignant (center and right) breast tumor cells.\"/></a>\n",
    "<p>FNA biopsy samples of benign (left) and malignant (center and right) breast tumor cells.</p>\n",
    "<p>Eickhoff, Carsten. (2014). Crowd-powered experts: helping surgeons interpret breast cancer images. ACM International Conference Proceeding Series. 53-56. 10.1145/2594776.2594788.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function displays the statistics of the features grouped by the class values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:35.357961Z",
     "start_time": "2021-02-01T12:50:34.341674Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_feature_statistics(df):\n",
    "    df.groupby(\"target\").boxplot(rot=90, layout=(1, 2), figsize=(12, 5), fontsize=7)\n",
    "\n",
    "\n",
    "show_feature_statistics(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, it can be observed that the attributes `mean area` and `worst area` have much larger ranges than other features have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Is it true that a feature with the larger range is a better feature? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c4db5e8ba428be653c58f92819cf9c0",
     "grade": true,
     "grade_id": "range",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize a numeric feature $Z$ to the unit interval as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z':= \\frac{Z}{b - a}\n",
    "\\end{align}\n",
    "$$ (min-max)\n",
    "\n",
    "where $a$ and $b$ are respectively the minimum and maximum possible values of $Z$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a$ and $b$ may be unknown in practice as the distribution of $Z$ is unknown. We perform the normalization on the samples: The min-max normalization of the sequence (in $i$) of $z_i$ is the sequence of\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z'_i := \\frac{z_i - \\min_j z_j}{\\max_j z_j - \\min_j z_j},\n",
    "\\end{align}\n",
    "$$ (min-max-sample)\n",
    "\n",
    "where $\\min_j z_j$ and $\\max_j z_j$ are respectively the minimum and maximum sample values. It follows that $0\\leq z'_i \\leq 1$ and the equalities hold with equality for some indices $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:35.363268Z",
     "start_time": "2021-02-01T12:50:35.359442Z"
    }
   },
   "outputs": [],
   "source": [
    "def minmax_normalize(df, suffix=\" (min-max normalized)\"):\n",
    "    \"\"\"Returns a new DataFrame with numerical attributes of the input DataFrame\n",
    "    min-max normalized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input to be min-max normalized. May contain both numeric\n",
    "        and categorical attributes.\n",
    "    suffix: string\n",
    "        Suffix to append to the names of normalized attributes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame:\n",
    "        A copy of df with its numeric attributes replaced by their min-max\n",
    "        normalization. The normalized features are renamed with the suffix\n",
    "        appended to the end of their original names.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # avoid overwriting the original dataframe\n",
    "    min_values = df.select_dtypes(include=\"number\").min()  # Skip categorical features\n",
    "    max_values = df[min_values.index].max()\n",
    "\n",
    "    # min-max normalize\n",
    "    df[min_values.index] = (df[min_values.index] - min_values) / (\n",
    "        max_values - min_values\n",
    "    )\n",
    "\n",
    "    # rename normalized features\n",
    "    df.rename(columns={c: c + suffix for c in min_values.index}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to rename the normalized features to differentiate them from the original features. The following plots the statistics of the normalized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:36.569473Z",
     "start_time": "2021-02-01T12:50:35.364574Z"
    }
   },
   "outputs": [],
   "source": [
    "df_minmax_normalized = minmax_normalize(df)\n",
    "assert df_minmax_normalized.target.to_numpy().base is df.target.to_numpy().base\n",
    "\n",
    "show_feature_statistics(df_minmax_normalized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalization, we can see how instances of different classes differ in different input features other than `mean area` and `worst area`. In particular, both `mean-concavity` and `worst-concavity` are substantially higher for malignant examples than for benign examples. Such details are hard to see in the plots before normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-max normalization is not appropriate for features with unbounded support where $b-a=\\infty$ in {eq}`min-max`. The normalization factor $\\max_j z_j - \\min_j z_j$ in {eq}`min-max-sample` for i.i.d. samples will approach $\\infty$ as the number of samples goes to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the distribution of each feature using [`displot`](https://seaborn.pydata.org/generated/seaborn.displot.html) provided by the package [`seaborn`](https://seaborn.pydata.org), which was imported with\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:36.787536Z",
     "start_time": "2021-02-01T12:50:36.571875Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact(\n",
    "    feature=dataset.feature_names, kernel_density_estimation=True, group_by_class=False\n",
    ")\n",
    "def plot_distribution(feature, kernel_density_estimation, group_by_class):\n",
    "    sns.displot(\n",
    "        data=df,\n",
    "        x=feature,\n",
    "        col=\"target\" if group_by_class else None,\n",
    "        kde=kernel_density_estimation,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the above widgets to check if the features appear to have unbounded support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a feature $Z$ with unbounded support, one may use the $z$-score/standard normalization instead:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z' := \\frac{Z - E[Z]}{\\sqrt{\\operatorname{Var}(Z)}}.\n",
    "\\end{align}\n",
    "$$ (standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the distribution of $Z$ is unknown, we normalize the sequence of i.i.d. samples $z_i$ using its sample mean $\\mu$ and standard deviation $\\sigma$ to the sequence of\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z'_i := \\frac{z_i - \\mu}{\\sigma}. \n",
    "\\end{align}\n",
    "$$ (standard-sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the function `standard_normalize` as follows:\n",
    "\n",
    "- Return a new copy of the input `DataFrame` `df` but with all its numeric attributes standard normalized. \n",
    "- You may use the methods `mean` and `std`.\n",
    "- Rename the normalized features by appending `suffix` to their names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.040118Z",
     "start_time": "2021-02-01T12:50:36.789409Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "debd4c1968ba5983d29f3a96859db798",
     "grade": false,
     "grade_id": "normalize-attributes",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def standard_normalize(df, suffix=\" (standard normalized)\"):\n",
    "    \"\"\"Returns a DataFrame with numerical attributes of the input DataFrame\n",
    "    standard normalized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input to be standard normalized. May contain both numeric\n",
    "        and categorical attributes.\n",
    "    suffix: string\n",
    "        Suffix to append to the names of normalized attributes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame:\n",
    "        A new copy of df that retains the categorical attributes but with the\n",
    "        numeric attributes replaced by their standard normalization.\n",
    "        The normalized features are renamed with the suffix appended to the end\n",
    "        of their original names.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "df_standard_normalized = standard_normalize(df)\n",
    "show_feature_statistics(df_standard_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.052101Z",
     "start_time": "2021-02-01T12:50:38.041836Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04b496bd07b3c05177f5daf0ed39b658",
     "grade": true,
     "grade_id": "test-standard-normalization",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert np.isclose(\n",
    "    df_standard_normalized.select_dtypes(include=\"number\").mean(), 0\n",
    ").all()\n",
    "assert np.isclose(df_standard_normalized.select_dtypes(include=\"number\").std(), 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daff4ae9be4ab1b656bb9d186062b798",
     "grade": true,
     "grade_id": "htest-standard-normalization",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a $k$-nearest-neighbor ($k$-NN) classifier, we can use `sklearn.neighbors.KNeighborsClassifier`. The following fits a $1$-NN classifier to the entire dataset and returns its training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.084674Z",
     "start_time": "2021-02-01T12:50:38.053818Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = df[dataset.feature_names], df.target\n",
    "kNN1 = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "kNN1.fit(X, Y)\n",
    "\n",
    "print(\"Training accuracy: {:0.3g}\".format(kNN1.score(X, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Why is the training accuracy for $1$-NN $100\\%$? Explain according to how 1-NN works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4af86ffab706c5d80300de98c370c2f",
     "grade": true,
     "grade_id": "1-NN",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overly-optimistic performance estimates, the following uses 10-fold cross validations to compute the accuracies of 1-NN trained on datasets with and without normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.322588Z",
     "start_time": "2021-02-01T12:50:38.086329Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "dfs = {\"None\": df, \"Min-max\": df_minmax_normalized}\n",
    "\n",
    "acc = pd.DataFrame(columns=dfs.keys())\n",
    "for norm in dfs:\n",
    "    acc[norm] = cross_val_score(\n",
    "        kNN1,\n",
    "        dfs[norm].loc[:, lambda df: ~df.columns.isin([\"target\"])],\n",
    "        # not [dataset.feature_names] since normalized features are renamed\n",
    "        dfs[norm][\"target\"],\n",
    "        cv=cv,\n",
    "    )\n",
    "\n",
    "acc.agg([\"mean\", \"std\"]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies appear to show that normalization improves the performance of 1-NN. More precisely, the improvement of $\\sim 5\\%$ in the accuracy appear statistically insignificance because it is at least twice the standard deviations of $\\sim 2\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Important**\n",
    "\n",
    "\n",
    "The proper way to compare performance should take statistical significance into account such as the [paired t-test](https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f). There is not much we can do to improve the statistical significance other than collecting more data. Repeating the cross-validation with different random seeds do not help as that only smooth out the randomness in splitting, not sampling. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies computed for the normalizations above suffer from a subtle issue that render them overly optimistic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Important**\n",
    "\n",
    "Since the normalization factors for cross validation were calculated from the entire dataset, the test data for each cross-validation fold may not be independent of the remaining normalized data for training the classifier. This subtle data leak may cause the performance estimate to be overly-optimistic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This issue can be resolved by computing the normalization factors from the training set instead of the entire dataset. To do so, we will create a pipeline using:\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "```\n",
    "\n",
    "- Like the filtered classifier in Weka, `sklearn.pipeline` provides the function `make_pipeline` to combine a filter with a classifier.\n",
    "- `sklearn.preprocessing` provides different filters for preprocessing features, , e.g., `StandardScaler` and `MinMaxScaler` for \n",
    "\n",
    "Creating a pipeline is useful especially for cross validation, where the normalization factors need to be recomputed for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.425410Z",
     "start_time": "2021-02-01T12:50:38.323742Z"
    }
   },
   "outputs": [],
   "source": [
    "kNN1_standard_normalized = make_pipeline(preprocessing.StandardScaler(), kNN1)\n",
    "acc[\"Standard\"] = cross_val_score(kNN1_standard_normalized, X, Y, cv=cv)\n",
    "acc[\"Standard\"].agg([\"mean\", \"std\"]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Similar to the above cell, correct the accuracies in `acc['Min-max']` to use `preprocessing.MinMaxScaler` as part of a pipeline for the 1-NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.545560Z",
     "start_time": "2021-02-01T12:50:38.426545Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe80c2cb5103bbe406d48e1c74c624df",
     "grade": false,
     "grade_id": "min-max",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "acc[\"Min-max\"].agg([\"mean\", \"std\"]).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.550933Z",
     "start_time": "2021-02-01T12:50:38.547115Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02df299a4ac68648a291b58a2764dd51",
     "grade": true,
     "grade_id": "test-min-max",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `sklearn` does not provide any function to plot the decision regions of a classifier, we provide the function `plot_decision_regions` in a module `util` defined in `util.py` of the current directory:\n",
    "\n",
    "```python\n",
    "from util import plot_decision_regions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.617405Z",
     "start_time": "2021-02-01T12:50:38.552492Z"
    },
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "plot_decision_regions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T13:11:42.307835Z",
     "start_time": "2021-03-20T13:11:42.002157Z"
    },
    "init_cell": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "Code(filename=\"util.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots the decision region for a selected pair of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:39.222608Z",
     "start_time": "2021-02-01T12:50:38.619309Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "if input(\"Execute? [y/N]\").lower() == \"y\":\n",
    "\n",
    "    @interact(\n",
    "        normalization=[\"None\", \"Min-max\", \"Standard\"],\n",
    "        feature1=dataset.feature_names,\n",
    "        feature2=dataset.feature_names,\n",
    "        k=widgets.IntSlider(1, 1, 5, continuous_update=False),\n",
    "        resolution=widgets.IntSlider(1, 1, 4, continuous_update=False),\n",
    "    )\n",
    "    def decision_regions_kNN(\n",
    "        normalization,\n",
    "        feature1=dataset.feature_names[0],\n",
    "        feature2=dataset.feature_names[1],\n",
    "        k=1,\n",
    "        resolution=1,\n",
    "    ):\n",
    "        scaler = {\n",
    "            \"Min-max\": preprocessing.MinMaxScaler,\n",
    "            \"Standard\": preprocessing.StandardScaler,\n",
    "        }\n",
    "        kNN = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "        if normalization != \"None\":\n",
    "            kNN = make_pipeline(scaler[normalization](), kNN)\n",
    "        kNN.fit(df[[feature1, feature2]].to_numpy(), df.target.to_numpy())\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        ax = plot_decision_regions(\n",
    "            df[[feature1, feature2]], df.target, kNN, N=resolution * 100\n",
    "        )\n",
    "        ax.set_title(\"Decision region for {}-NN\".format(k))\n",
    "        ax.set_xlabel(feature1)\n",
    "        ax.set_ylabel(feature2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interact with the widgets to: \n",
    "\n",
    "- Learn the effect on the decision regions/boundaries with different normalizations and choices of $k$.\n",
    "- Learn to draw the decision boundaries for $1$-NN with min-max normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeated computations, `plot_decision_regions` is a decorated function with its return values memorized/cached. (For details, see a [notebook](https://www.cs.cityu.edu.hk/~ccha23/cs1302book/Lecture6/Decorator.html) and a [tutorial](https://realpython.com/primer-on-python-decorators/) on decorator.) \n",
    "\n",
    "To clear the cached plots, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:39.233480Z",
     "start_time": "2021-02-01T12:50:39.223816Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_decision_regions.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the following code to plot the decision regions for decision trees. Afterwards, explain whether the decision regions change for different normalizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:39.570095Z",
     "start_time": "2021-02-01T12:50:39.234721Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b055a505d1ab5c763e150a438d5e2b2",
     "grade": false,
     "grade_id": "DT-region",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "if input(\"Execute? [y/N]\").lower() == \"y\":\n",
    "\n",
    "    @interact(\n",
    "        normalization=[\"None\", \"Min-max\", \"Standard\"],\n",
    "        feature1=dataset.feature_names,\n",
    "        feature2=dataset.feature_names,\n",
    "        resolution=widgets.IntSlider(1, 1, 4, continuous_update=False),\n",
    "    )\n",
    "    def decision_regions_kNN(\n",
    "        normalization,\n",
    "        feature1=dataset.feature_names[0],\n",
    "        feature2=dataset.feature_names[1],\n",
    "        resolution=1,\n",
    "    ):\n",
    "        scaler = {\n",
    "            \"Min-max\": preprocessing.MinMaxScaler,\n",
    "            \"Standard\": preprocessing.StandardScaler,\n",
    "        }\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        ax = plot_decision_regions(\n",
    "            df[[feature1, feature2]], df.target, DT, N=resolution * 100\n",
    "        )\n",
    "        ax.set_title(\"Decision region for Decision Tree\")\n",
    "        ax.set_xlabel(feature1)\n",
    "        ax.set_ylabel(feature2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f006f32a0176204e7bba0f6eb5a48ff3",
     "grade": true,
     "grade_id": "DT-decision-regions",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "beige"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
